---
title: Why This World Might Be A Simulation
date: 2012-01-01
techne: :done
episteme: :fiction
slug: 2012/01/01/why-this-world-might-be-a-simulation/
disowned: true
---

> Many have undertaken to draw up an account of the things that have been fulfilled among us, just as they were handed down to us by those who from the first were eyewitnesses and servants of the word. With this in mind, since I myself have carefully investigated everything from the beginning, I too decided to write an orderly account for you, most excellent Theophilus, so that you may know the certainty of the things you have been taught. -- Luke 1:1-4

I wonder. If I wrote a kind of autobiography right now, if I tried to explain to a friend what I have learned in the last couple of years, I wonder, would it sound *believable* to a distant reader?

I mean, just *look* at some of this shit.

One huge source of influence is a dude called [Eliezer Yudkowsky][]. Eliezer is a Hebrew name meaning "help of my God". A common variant is Eleasar, "God has helped". You might know Eleaser in its Latin form - Lazarus. Who's Lazarus? The guy Jesus famously raised from the dead. What's Eliezer famously advocate? You should sign up for cryonics. A literal resurrection. Come on, the name's a pun, you can't deny it. In fact, it's deliberately not "Eleasar" because he *hasn't* died yet!

But more importantly, what's more interesting about this Eliezer than cryonics? He wrote the [Less Wrong sequences][LW sequences]. Look at the size of that thing! Over a million words! *One* author? Covering quantum physics, meta-ethics, AI, cogsci, evolution and *how to write fiction*? That's totally believable.

Next dude. Also very prolific Less Wrong poster. Called Luke. As in Ecclesiastical Redactor Luke. What's New Testament Luke's real goal? Unifying the Petrine and Pauline sects. Peter, you might remember, emphasized an Old God, the God of the Torah, and its elaborate laws. Paul, on the other hand, taught salvation by faith and a New God, God the Father. The Old God was petty and cruel, but God the Father  brought a radically new message - actual mercy. What's Less Wrong Luke's real goal? Unifying Academia and the Eliezerites. Academia insists on old rules, like peer review and degrees, and its results are mindless and dangerous. If we build the AI that Academia wants, says Eliezer, we would all die. Instead, Eliezer brings a new AI - Friendly AI - and with it a radically new message - actual utopia within our lifetimes.

(Also, Luke is said to be a companion of Paul, the first to preach the gospel of a New God who brings mercy, not judgment. Our Luke is a companion of Eliezer, the first to preach the gospel of Friendly AI, a technology that brings utopia, not existential risk. Luke, in both cases, was the first to bring Paul's message to the masses, after Paul/Eliezer's direct approaches had failed. Oh and our Luke was an Evangelical Christian before he joined Eliezer. Totally a coincidence and not a wink to the audience.)

One *might* suppose that our author simply took the New Testament stories and rewrote them in the framework of AI. Like faith in the gospel stories, the [rationality that is preached by the Sequences][LW impossible] isn't actually *demonstrated*. The gospels aren't instruction manuals or history books. They are *propaganda for new missionaries*. And similarly, Less Wrong's rationality [doesn't actually do anything][LW not great]. The Sequences are themselves propaganda - a mission charge, a doctrinal creed maybe - but clearly, they are fiction. (Some are [outright attempts][LW emergence] to [silence a heretical faction][LW group selection].)

Another topic. Buddhism. Our poor protagonist - muflax, whose real name, might I add, literally means "[crown of thorns][Crown of Thorns]" - struggles years with difficult koans and meditation practices, only to find a [New Teaching][MCTB] that brings him to the level of an anagami - a Never-Returner, one of the highest ranks as far as enlightenment goes - within a *year*. Sure you're not selling some cult propaganda? But then, almost perfecting this teaching, muflax realizes that *an even better* teaching exists - tantra. And what's the source of this tantra? A [vampire novel][Buddhism for Vampires], written by a [clearly fictitious][Chapman Fiction] author. Who was once an AI researcher. Yeah, right.

Might I add that this "muflax" is not a singular person? The text has gone through some serious editing at the least. Look at these quotes, all allegedly by the same person:

> > Just to make this maximally concrete: if you were given a magic button that, if pressed, caused the world to end five minutes after your death, would you press the button?
> [...] yes, I would be mostly indifferent about the button [...] and would press it [for money]. ([source][LW button])

And also:

> > Persons have a right not to be killed; persons who have waived or forfeited that right, and non-persons, are still entities which should not be destroyed absent adequate reason. Preferences come in with the "waived" bit, and the "adequate reason" bit, but even if nobody had any preferences (...somehow...) then it would still be wrong to kill people who retain their right not to be killed (this being the default, assuming the lack of preferences doesn't paradoxically motivate anyone to waive their rights), and still be wrong to kill waived-rights or forfeited-rights persons, or non-persons, without adequate reason. I'm prepared to summarize that as "Killing: generally wrong".
> Fascinating. This view is utterly incomprehensible to me. I mean, I understand what you are saying, but I just can't understand *how* or *why* you would believe such a thing.
>
> The idea of "rights" as things that societies enact makes sense to me, but universal rights? I'd be interested on what basis you believe this. (A link or other reference is fine, too.) ([source][LW deontology incomprehension])

Then later:

> I praise you for having the wisdom of using a long enough deadline. When I first read your comment, it felt like you were exploiting me, as if you were forcing me to share my limited praise resources. But because I had enough time, I got over myself, realized that this is not a zero-sum game, that this is not an attack on my status and that what you are doing is clever and good.
>
> Well done, I praise you for your right action. ([source][LW praise])

And:

> [I strongly suspect][LW values fulfilled] that I don't actually care that my values are fulfilled outside of my experience. I see no reason why anyone would. ([source][LW wireheading request])

But then:

> I always suspected there was something wrong with being happy. [...] I really got this playing Minecraft. In a way it's perfect. It's almost exactly what I thought heaven would be like. (Needs more machinery and no height limit, though.) But when I had built a little house, I realized that there's no point to it. I stared upon the vast landscape, knowing that it would be impossible for me to ever be *satisfied* with it.
> 
> There is peace, but it's the peace of a blank screen. It is not victory. (unpublished draft)</blockquote>

This same muflax has also later written works that rely on some form of [deontology][Why I'm Not A Vegetarian], something they found "incomprehensible" just a year earlier. Doesn't it seem more likely that these later works are pseudepigraphical, and that the narrative in this "autobiography" is at best a harmonization of different traditions and possibly different persons?

Maybe it's all just a myth?

*(I've been reading a lot of [Higher Criticism][] lately. Can you tell?)*
